# KoBART λ…Όλ¬Έ μ”μ•½ λ¨λΈ ν•™μµ ν”„λ΅μ νΈ

μ΄ ν”„λ΅μ νΈλ” ν•κµ­μ–΄ λ…Όλ¬Έ μ „λ¬Έμ„ μ…λ ¥μΌλ΅ λ°›μ•„ ν•µμ‹¬ λ‚΄μ©μ„ μ”μ•½ν•λ” KoBART κΈ°λ°μ ν…μ¤νΈ μ”μ•½ λ¨λΈμ„ ν•™μµν• μ‘μ—…μ…λ‹λ‹¤. 

μ΄ 3κ°μ μ£Όμ” λ…ΈνΈλ¶μ„ ν†µν•΄ λ°μ΄ν„° μ „μ²λ¦¬, λ¨λΈ ν•™μµ, PEFT(LORA) μ μ© μ‹¤ν—μ„ λ‹¨κ³„μ μΌλ΅ μν–‰ν–μµλ‹λ‹¤.

---

## π“ λ…ΈνΈλ¶ κµ¬μ„±

| λ…ΈνΈλ¶ μ΄λ¦„ | μ„¤λ… |
|-------------|------|
| `dataset.ipynb` | μ›λ³Έ JSON λ°μ΄ν„°λ¥Ό ν† ν¬λ‚μ΄μ¦ν•κ³  ν•™μµμ© λ°μ΄ν„°μ…‹μΌλ΅ μ €μ¥ |
| `kobart-peft-summary.ipynb` | LoRA κΈ°λ°μ PEFT κΈ°λ²•μ„ μ μ©ν•μ—¬ νλΌλ―Έν„° ν¨μ¨μ  λ―Έμ„Έμ΅°μ • μν–‰ |

---

## π“ λ°μ΄ν„°μ…‹ μ „μ²λ¦¬ λ°©μ‹

- μ‚¬μ© λ°μ΄ν„°: AI Hub ν•κµ­μ–΄ λ…Όλ¬Έ μ”μ•½ λ°μ΄ν„°
- μ›λ¬Έ: `original_text`, μ”μ•½: `summary_text`
- ν† ν¬λ‚μ΄μ €: `PreTrainedTokenizerFast` from `gogamza/kobart-base-v2`
- `max_length=512`λ΅ truncation λ° padding μ μ©
- μµμΆ… λ°μ΄ν„°μ…‹μ€ HuggingFace `Dataset` ν¬λ§·μΌλ΅ μ €μ¥ ν›„ ν•™μµμ— μ‚¬μ©

---

## π§  λ¨λΈ ν•™μµ μ„¤μ •

| ν•­λ© | κ°’ |
|------|----|
| Base model | `gogamza/kobart-base-v2` |
| ν•™μµ μ—ν­ | 3 (PEFT μ‹¤ν—) |
| λ°°μΉ μ‚¬μ΄μ¦ | 32 |
| Learning Rate | 5e-5 |
| ν•™μµ μ¥λΉ„ | Kaggle P100 |

---

## π§ PEFT (LoRA) μ‹¤ν—

- `peft`, `transformers`, `accelerate` λΌμ΄λΈλ¬λ¦¬ ν™μ©
- LoraConfig μ„¤μ •:
  ```python
  LoraConfig(
      r=8,
      lora_alpha=32,
      lora_dropout=0.1,
      task_type=TaskType.SEQ_2_SEQ_LM
  )
  ```
- ν•™μµ νλΌλ―Έν„° μ: μ „μ²΄ λ€λΉ„ μ•½ 0.35%λ§ ν•™μµ

---

## π“ μ„±λ¥ κ²°κ³Ό

ν…μ¤νΈ λ°μ΄ν„° 100κ°μ— λ€ν• κ²°κ³Ό

| λ©”νΈλ¦­ | κ°’ |
|--------|----|
| ROUGE-1 | μ•½ 0.17 |
| ROUGE-2 | μ•½ 0.04 |
| ROUGE-L | μ•½ 0.17 |
| BERTScore (F1) | 0.7853 |

> λ¨λΈμ€ μΌλ¶€ λ¬Έμ¥μ—μ„ μ›λ¬Έμ λ¬Έκµ¬λ¥Ό κ·Έλ€λ΅ λ°λ³µν•κ±°λ‚, μ£Όμ λ¥Ό λ…ν™•ν•κ² μ”μ•½ν•μ§€ λ»ν•λ” κ²½ν–¥μ΄ μμ

---

## β… ν–¥ν›„ κ°μ„  λ°©ν–¥

- μ¶”λ΅  μ‹ `length_penalty` λ° `no_repeat_ngram_size` μ΅°μ •
- μ „μ΄ ν•™μµμ„ μ„ν• λ‰΄μ¤ μ”μ•½ β†’ λ…Όλ¬Έ μ”μ•½ λ°©μ‹ μ‹λ„

---

## π–ΌοΈ μ”μ•½ κ²°κ³Ό

| ![image](https://github.com/user-attachments/assets/3f5001b6-b92c-44bc-8a69-16b724e9a2b2)

